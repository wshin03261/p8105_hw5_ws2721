---
title: "HW5"
output: github_document
date: "2024-11-13"
---

```{r}
library(tidyverse)
library(broom)

set.seed(1)
```

# Problem 1

```{r}
duplicate = function(n) {
  birthday = sample(1:365, n, replace = TRUE)
  return(length(birthday) != length(unique(birthday)))
}
```

I will first write a function that randomly assigns n number of people a birthday. Then checks the if a person has a same birthday and show a result.

True: there is duplicate birthday
False: there aren't any duplicate birthday

```{r}
group_sizes = 2:50
probabilities = numeric(length(group_sizes))
```

Have a group sizes 2 through 50.
Make a variable that records the probabilities for each group.

```{r}
for (i in 1:length(group_sizes)) {
  groups = group_sizes[i]
  
  results = replicate(10000, duplicate(groups))
  
  probabilities[i] = mean(results)
}
```

Make a function that run the simulation for each group size and run the simulation 10000 times for each group and calculate the probability of at least one duplicate birthday.

```{r}
data = data_frame(GS = group_sizes, P = probabilities)

ggplot(data, aes(x = GS, y = P)) +
  geom_smooth() +
  geom_point() +
  labs(
    title = "Probability vs. Group Size",
    x = "Group Size",
    y = "Probability of Duplicate Birthday"
  ) +
  theme_minimal()
```

It shows a increasing curve as group size increases the probability increases.

# Problem 2

```{r}
mus = 0:6
alpha = 0.05

results = data.frame()
```

I've set the parameters and storage for the results.

```{r}
for (mu in mus) {
  for (i in 1:5000) {
    
    sample_data = rnorm(30, mean = mu, sd = 5)
    
    t_test = broom::tidy(t.test(sample_data, mean = 0))
    
    results = rbind(results, data.frame(
      mu_true = mu,
      mu_hat = mean(sample_data),
      p_value = t_test$p.value,
      rejected = t_test$p.value < alpha
    ))
  }
}

power_results = results |> 
  group_by(mu_true) |> 
  summarize(power = mean(rejected))

estimated_results = results |> 
  group_by(mu_true) |> 
  summarize(avg_mu_hat = mean((mu_hat)),
            avg_mu_hat_rejected_abs = mean(abs(mu_hat[rejected])),
            avg_mu_hat_rejected = mean(mu_hat[rejected]))
```

Then, run a loop that will conduct the t-test with the null hypothesis of mu = 0.
Each iteration, it will test 5000 times with sample data with n = 30, standard deviation = 5, consists of random mean that is from true mean of 0 through 6.
Then I made 2 data frame that calculates power and another data frame that has average of all sample mean by their true mean, average absolute value of the rejected mean by their true mean, average value of the rejected mean by their true mean.
The reason I made the mu_hat average to be absolute value is that negative values could even out the numbers to be closer to true mu, therefore hard to see true parameters of the mean rejected values.

```{r}
ggplot(power_results, aes(x = mu_true, y = power)) +
  geom_line() +
  geom_point() +
  labs(x = "True mu", y = "Power", title = "Power vs. Effect Size (True mu)")
```

The graph shows that as effect size increases, the power increases. This makes sense because there will be a higher chance of the null hypothesis (mu = 0) of the t-test being rejected if the true mean of the sample increases.It shows the highest increase between the true mean of 1 to 3.

```{r}
ggplot(estimated_results, aes(x = mu_true)) +
  geom_line(aes(y = avg_mu_hat), color = "blue", linetype = "solid", size = 1) +
  geom_point(aes(y = avg_mu_hat), color = "blue") +
  geom_line(aes(y = avg_mu_hat_rejected_abs), color = "red", linetype = "dashed", size = 1) +
  geom_point(aes(y = avg_mu_hat_rejected_abs), color = "red") + 
  geom_line(aes(y = avg_mu_hat_rejected), color = "black", linetype = "dotted", size = 1) +
  geom_point(aes(y = avg_mu_hat_rejected), color = "black") +
  labs(x = "True mu", y = "Average Estimate of mu",
       title = "Average Estimate of mu (Blue: All, Red: Rejected(abs), Black: Rejected)")
```

The Blue line shows the mean of sample average.
The Red line shows the absolute value of the mean of the null rejected sample average.
The Black line shows the mean of the null rejected sample average.

For alpha = 0.05 for two tailed test, if the z-value exceeds 1.96 or -1.96, they reject the null hypothesis.
If we apply this concept to the result of the graph, it explains the graph.
z-value equation for sample size bigger than or equal to 30: z = (x-mu) / (sigma/sqrt(n))
Since we know the rejection z -value for alpha = 0.05, x, sigma, n we can calculate the absolute value of mu to reject the null hypothesis.

z = 1.96, sigma = 5, n = 30

Absolute value:
For x = 0, mu = 1.789
The red graph the average estimate of mu is all above 1.789. 

The blue graph is self-explanatory, as True mu increases, the average estimate of mu follows.
The Black graph, True mu = 0 has null rejected sample average of around 0. If we think about that, because the true mu is 0 and sample is normally distributed. The rejected mean is calculated of both negative side and positive side and it shows a value close to 0. Starting from True mu = 1, it starts to follow the red graph as it has significantly lower chance to reject the null as a negative value.

# Problem 3

```{r}
homicide_data = read_csv("homicide-data.csv") |> 
  mutate(city_state = paste(city, state, sep = ", "))

homicide = homicide_data |> 
  group_by(city_state) |> 
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )
```

The raw data consists of unique ID, reported date, the information of the victim, location, and the status of the case resolution (disposition). I have merged city and state to make another variable called city_state.

Then. I have made a data frame called "homicide" that counts all the cases and unsolved cases that happended in the location of which city and state.

```{r}
baltimore = homicide |> 
  filter(city_state == "Baltimore, MD")

baltimore_prop_test = broom::tidy(prop.test(baltimore$unsolved_homicides, baltimore$total_homicides))

baltimore_estimate = baltimore_prop_test |> 
  select(estimate, conf.low, conf.high)

baltimore_estimate
```

I first filtered the location of interest, Baltimore, MD. Then, conducted the proportion test to get the estimate and the confidence interval.

The estimate is 0.646 with the confidence interval between 0.628 and 0.663.

```{r}
city_proportions = homicide |> 
  mutate(
    prop_test = map2(unsolved_homicides, total_homicides, ~ prop.test(.x, .y) |> 
                       broom::tidy())
  ) |>  
  unnest(prop_test) |> 
  select(city_state, estimate, conf.low, conf.high) |> 
  arrange(desc(estimate))

ggplot(city_proportions, aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City, State",
    y = "Proportion of Unsolved Homicides"
  ) +
  coord_flip() +
  theme_minimal()
```

Now, I applied the proportion test to every location in the data. 
As instructed, I did this within a “tidy” pipeline, making use of map2, list columns and unnest to create a tidy dataframe with estimated proportions and confidence intervals for each city.

From the plot, we can see that Chicago, IL has the highest proportion of unsolved homicides and Richmond, VA the lowest, I didn't consider Tulsa, AL as a lowest because it only had 1 case with 0 unsolved homicide. Tulsa, AL didn't have not enough information.
